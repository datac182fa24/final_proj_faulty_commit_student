{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data C182 Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# IMPORTANT: change to your own root folder path! This dir should contain the contents of:\n",
    "# https://github.com/datac182fa24/final_proj_faulty_commit_student\n",
    "root_folder = \"/content/drive/MyDrive/teaching/data_c182_fa2024/final_project/final_proj_faulty_commit_student/\"\n",
    "import os\n",
    "os.chdir(root_folder)\n",
    "# validate that we're in the right directory (ls should show files like: train.py, README.md, etc)\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: enable autoreload so that changes to .py files are auto-imported. For convenience.\n",
    "#   https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set()\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize submission data structure\n",
    "# `auto_grader_data`: your notebook will populate this dict with important outputs that the \n",
    "#   autograder will check.\n",
    "# IMPORTANT: do not add additional read/write calls to `auto_grader_data` in your own code.\n",
    "#   Leave the existing provided read/write calls untouched.\n",
    "#   Violations will be flagged as academic dishonesty.\n",
    "# WARNING: running this cell will clear the state in `auto_grader_data`. If you run this\n",
    "#   cell, you'll have to run the rest of the notebook.\n",
    "from utils.utils import add_entry_to_auto_grader_data\n",
    "auto_grader_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data preprocessing\n",
    "\n",
    "In this part, you will implement the dataloader and data preprocessors. See the project spec for detailed instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.A) Implement `compute_data_preprocessor()`\n",
    "\n",
    "In `dataloader/fault_csv_dataset.py`, you will implement the `compute_data_preprocessor()` function. See the project spec for detailed instructions.\n",
    "\n",
    "Once you've implemented this function correctly, the following cell should succeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.dataloader.test_fault_csv_dataset import TestComputeDataPreprocessor\n",
    "\n",
    "testcase = TestComputeDataPreprocessor()\n",
    "testcase.test_simple()\n",
    "print(\"Passed tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.B) Implement `FaultCSVDataset`.\n",
    "\n",
    "Once you've correctly implemented this, the following test case should succeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.dataloader.test_fault_csv_dataset import TestFaultCsvDataset\n",
    "\n",
    "testcase = TestFaultCsvDataset()\n",
    "testcase.test_simple()\n",
    "print(\"Passed tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.C) Benchmark your Dataloader\n",
    "\n",
    "We will benchmark your dataloader. To receive full credit, you must achieve a throughput of at least `5000 rows/sec`. The below cell should output something like:\n",
    "```\n",
    "features_preproc shape=torch.Size([43505, 1098])\n",
    "Created FaultCSVDataset (1.4087190628051758 secs)\n",
    "benchmark_stats: {'throughput_rows_per_sec': 33911.9315827453, 'total_time_secs': 0.11795258522033691, 'latency_secs_avg': 0.00011732101440429687, 'latency_secs_std': 3.2681162353353425e-05}\n",
    "[1/10877] row_dict={'features': tensor([[-0.3203,  0.0000, -0.0354,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.3203,  0.0000, -0.0349,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.3203,  0.0000, -0.0356,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.3203,  0.0000, -0.0348,  ...,  0.0000,  0.0000,  0.0000]]), 'label': tensor([0, 0, 0, 0])}\n",
    "[2/10877] row_dict={'features': tensor([[ 3.1216,  0.0000, -0.0101,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.3203,  0.0000, -0.0354,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.3203,  0.0000, -0.0351,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.3203,  0.0000, -0.0353,  ...,  0.0000,  0.0000,  0.0000]]), 'label': tensor([0, 0, 0, 0])}\n",
    "[3/10877] row_dict={'features': tensor([[-0.3203,  0.0000, -0.0348,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.3203,  0.0000, -0.0351,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.3203,  0.0000, -0.0354,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.3203,  0.0000, -0.0353,  ...,  0.0000,  0.0000,  0.0000]]), 'label': tensor([0, 0, 0, 0])}\n",
    "[4/10877] row_dict={'features': tensor([[-0.3203,  0.0000, -0.0337,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.3203,  0.0000, -0.0272,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.3203,  0.0000, -0.0233,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.3203,  0.0000, -0.0356,  ...,  0.0000,  0.0000,  0.0000]]), 'label': tensor([0, 0, 0, 0])}\n",
    "Your dataloader throughput was: 33911.9315827453\n",
    "Is larger than 5000 rows/sec? True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.benchmark_dataloader import main_benchmark_dataloader\n",
    "benchmark_stats_pt1c = main_benchmark_dataloader()\n",
    "print(f\"Your dataloader throughput was: {benchmark_stats_pt1c['throughput_rows_per_sec']}\")\n",
    "print(f\"Is larger than 5000 rows/sec? {benchmark_stats_pt1c['throughput_rows_per_sec'] >= 5000}\")\n",
    "add_entry_to_auto_grader_data(auto_grader_data, [\"output\", \"part_1_c\", \"dataloader_throughput_rows_per_sec\"], benchmark_stats_pt1c[\"throughput_rows_per_sec\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Modeling\n",
    "In this section, you will train several models on the training dataset, using the dataloader you've implemented.\n",
    "\n",
    "All models will implement the `FaultyCommitClassifierModel` interface (modeling.model_interface.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.A) SingleLayerNN\n",
    "As a warmup, implement the `SingleLayerNN` class (modeling/single_layer_nn.py). This is essentially implementing logistic regression as a NN. Read the class documentation for more information.\n",
    "\n",
    "If you've implemented it correctly, the following test should pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.modeling.test_single_layer_nn import TestSingleLayerNN\n",
    "\n",
    "testcase = TestSingleLayerNN()\n",
    "testcase.test_simple()\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.B) Implement `ClassificationTrainer`\n",
    "Next, implement the `ClassificationTrainer` (in: trainer/trainer.py). Notably, this class contains the training loop, which you will implement.\n",
    "\n",
    "Tip: feel free to use the training loops from previous homework assignments as reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.C) Train a SingleLayerNN\n",
    "To demonstrate that your dataloader + train loop is working correctly, you will run the `train_and_eval()` function (in train.py). Run the following cell, it should output something like:\n",
    "```\n",
    "Train Epoch=4/5 Batch=0/170 loss=0.1745077669620514 train_accuracy=0.9453125 train_accuracy_pos_class=0.20000001788139343                                     | 0/170 [00:00<?, ?it/s] \n",
    "GPU max_memory_allocated: 19.313664 MB\n",
    "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170/170 [00:00<00:00, 351.24it/s] \n",
    "Skipping validation (self.skip_val=True)████████████████████████████████████████████████████████████████████████████████▎                         | 139/170 [00:00<00:00, 357.00it/s] \n",
    "(val) epoch=5/5: AP=0.00000 (T=1.00000) precision@T=0.00000 recall@T=0.00000 f1@T=0.00000\n",
    "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.89it/s] \n",
    "Finished training! 2.652967691421509 secs (total_num_epochs=5)\n",
    "Skipping test (skip_test=True)\n",
    "```\n",
    "To get full credit, your implementation should get a final train loss less than 0.35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import main_train_pt2c\n",
    "\n",
    "train_meta_pt2c, fig_pt2c = main_train_pt2c()\n",
    "print(f\"Final train loss: {train_meta_pt2c.losses[-1]}\")\n",
    "print(f\"Is less than 0.35? {train_meta_pt2c.losses[-1] <= 0.35}\")\n",
    "add_entry_to_auto_grader_data(auto_grader_data, [\"output\", \"part_2_c\", \"single_layer_nn_train_loss\"], train_meta_pt2c.losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Offline Evaluation\n",
    "\n",
    "In this section, you will implement the offline evaluation pipeline for the binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.A) Implement `predict_samples()`\n",
    "Implement the `predict_samples()` function (in: evaluation/offline_eval.py). This function takes a model, generates model predictions on an evaluation dataset, and stores the inference results into a struct `PredictionMetadata`.\n",
    "\n",
    "To test your implementation, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.evaluation.test_offline_eval import TestPredictSamples\n",
    "\n",
    "testcase = TestPredictSamples()\n",
    "testcase.test_batchsize_1()\n",
    "testcase.test_batchsize_3()\n",
    "testcase.test_batchsize_4()\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.B) Implement `compute_eval_metrics()`\n",
    "Next, implement the `compute_eval_metrics()` function (in: evaluation/offline_eval.py). Given model predictions and ground truth labels (aka the output of `predict_samples()`), this function will calculate the evaluation metrics that we care about.\n",
    "\n",
    "To test your implementation, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.evaluation.test_offline_eval import TestComputeEvalMetrics\n",
    "testcase = TestComputeEvalMetrics()\n",
    "testcase.test_simple()\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.C) Evaluate trivial baselines\n",
    "Implement the above three trivial models (`AlwaysPositiveBinaryClassifier`, `AlwaysNegativeBinaryClassifier`, `RandomBinaryClassifier`), and run the corresponding notebook to test your implementations. Your implementation should output something extremely close to:\n",
    "\n",
    "<details>\n",
    "<summary> Click me for desired output </summary>\n",
    "```\n",
    "Loaded train dataset from data/split/faulty_commit_dev_train.csv, num_train_rows=43505 dim_input_feats: 1098\n",
    "======== Test metrics (RandomBinaryClassifier) ========\n",
    "100%|██████████| 189/189 [00:00<00:00, 379.87it/s]\n",
    "(Test) AP=0.01375 (T=1.00000) precision@T=0.01386 recall@T=0.50379 f1@T=0.02697\n",
    "features_preproc shape=torch.Size([43505, 1098])\n",
    "features_preproc shape=torch.Size([29003, 1098])\n",
    "features_preproc shape=torch.Size([48337, 1098])\n",
    "Loaded train dataset from data/split/faulty_commit_dev_train.csv, num_train_rows=43505 dim_input_feats: 1098\n",
    "======== Test metrics (AlwaysPositiveBinaryClassifier) ========\n",
    "100%|██████████| 189/189 [00:00<00:00, 236.68it/s]\n",
    "(Test) AP=0.01363 (T=1.00000) precision@T=0.01363 recall@T=1.00000 f1@T=0.02690\n",
    "======== Test metrics (AlwaysNegativeBinaryClassifier) ========\n",
    "100%|██████████| 189/189 [00:00<00:00, 241.45it/s]\n",
    "(Test) AP=0.01363 (T=0.00000) precision@T=0.01363 recall@T=1.00000 f1@T=0.02690\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.evaluate_trivial_models import main_evaluate_model_random, main_evaluate_always_pos_neg\n",
    "test_eval_metrics_pt3c_random = main_evaluate_model_random()\n",
    "test_eval_metrics_pt3c_always_pos, test_eval_metrics_pt3c_always_neg = main_evaluate_always_pos_neg()\n",
    "add_entry_to_auto_grader_data(auto_grader_data, [\"output\", \"part_3_c\", \"random_binary_classifier\", \"test_eval_metrics\"], test_eval_metrics_pt3c_random.to_pydict())\n",
    "add_entry_to_auto_grader_data(auto_grader_data, [\"output\", \"part_3_c\", \"always_pos_classifier\", \"test_eval_metrics\"], test_eval_metrics_pt3c_always_pos.to_pydict())\n",
    "add_entry_to_auto_grader_data(auto_grader_data, [\"output\", \"part_3_c\", \"always_neg_classifier\", \"test_eval_metrics\"], test_eval_metrics_pt3c_always_neg.to_pydict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Train and eval SingleLayerNN\n",
    "Once (Part 1), (Part 2), (Part 3) are done, let's put all of the parts together and run the full train+eval pipeline! Run the following notebook cells, it will train a `SingleLayerNN` model on the training dataset, measure eval metrics on the validation dataset after each training epoch, and then evaluate on the test set at the end of training. Then, it will generate various plots (eg: train loss over time, eval metrics on val/test sets).\n",
    "\n",
    "To receive full credit, you should achieve a test AP of greater than 0.015.\n",
    "\n",
    "If everything is implemented correctly (dataloading, model architecture, and evaluation pipeline), then you should receive something like this:\n",
    "```\n",
    "(val) epoch=10/10: AP=0.05734 (T=0.32974) precision@T=0.11614 recall@T=0.19283 f1@T=0.14496\n",
    "Finished training! 12.00160551071167 secs (total_num_epochs=10)\n",
    "======== Validation metrics ========\n",
    "[epoch=1/10] (val) AP=0.03989 (T=0.48066) precision@T=0.09879 recall@T=0.18089 f1@T=0.12779\n",
    "[epoch=2/10] (val) AP=0.04018 (T=0.42750) precision@T=0.09364 recall@T=0.18601 f1@T=0.12457\n",
    "[epoch=3/10] (val) AP=0.04002 (T=0.38345) precision@T=0.09706 recall@T=0.16894 f1@T=0.12329\n",
    "[epoch=4/10] (val) AP=0.04081 (T=0.36203) precision@T=0.09446 recall@T=0.15700 f1@T=0.11795\n",
    "[epoch=5/10] (val) AP=0.04321 (T=0.33020) precision@T=0.09735 recall@T=0.16894 f1@T=0.12352\n",
    "[epoch=6/10] (val) AP=0.04669 (T=0.31828) precision@T=0.10200 recall@T=0.18259 f1@T=0.13089\n",
    "[epoch=7/10] (val) AP=0.04916 (T=0.29781) precision@T=0.10388 recall@T=0.19625 f1@T=0.13585\n",
    "[epoch=8/10] (val) AP=0.05088 (T=0.31498) precision@T=0.11187 recall@T=0.19454 f1@T=0.14206\n",
    "[epoch=9/10] (val) AP=0.05444 (T=0.32641) precision@T=0.11626 recall@T=0.19283 f1@T=0.14506\n",
    "[epoch=10/10] (val) AP=0.05734 (T=0.32974) precision@T=0.11614 recall@T=0.19283 f1@T=0.14496\n",
    "======== Test metrics ========\n",
    "100%|██████████| 48/48 [00:00<00:00, 60.82it/s]\n",
    "/content/drive/MyDrive/teaching/data_c182_fa2024/final_project/final_proj_faulty_commit_sol/evaluation/offline_eval.py:136: RuntimeWarning: invalid value encountered in divide\n",
    "  f1s = 2.0 * (precisions * recalls) / (precisions + recalls)\n",
    "(Test) AP=0.02361 (T=0.60950) precision@T=0.07212 recall@T=0.07891 f1@T=0.07536\n",
    "Using validation set operating point threshold T=0.3297363519668579:\n",
    "(Test) (T=0.32974) precision@T=0.03173 recall@T=0.18665 f1@T=0.05424\n",
    "Saving figure to: submission/main_train_pt4.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import main_train_and_eval_pt4\n",
    "\n",
    "train_meta_pt4, (test_eval_metrics_pt4, test_metrics_op_pt4), fig_pt4 = main_train_and_eval_pt4()\n",
    "add_entry_to_auto_grader_data(auto_grader_data, [\"output\", \"part_4_a\", \"single_layer_nn\", \"train_meta\"], train_meta_pt4.to_pydict())\n",
    "add_entry_to_auto_grader_data(auto_grader_data, [\"output\", \"part_4_a\", \"single_layer_nn\", \"test_eval_metrics\"], test_eval_metrics_pt4.to_pydict())\n",
    "add_entry_to_auto_grader_data(auto_grader_data, [\"output\", \"part_4_a\", \"single_layer_nn\", \"test_metrics_op\"], test_metrics_op_pt4.to_pydict())\n",
    "print(f\"Test AP {test_eval_metrics_pt4.average_precision:.5f} >= 0.015? {test_eval_metrics_pt4.average_precision >= 0.015}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Train a better model\n",
    "\n",
    "Challenge: implement a new model architecture that outperforms the `SingleLayerNN` model you implemented. To achieve full credit, your model must achieve greater than 0.025 test AP.\n",
    "\n",
    "Tip: implement your model in modeling/, then modify `train_and_eval_pt5()` (in train_pt5.py) to train+eval your new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_pt5 import train_and_eval_pt5\n",
    "\n",
    "train_metadata_pt5, (test_eval_metrics_pt5, test_metrics_op_pt5), fig = train_and_eval_pt5()\n",
    "\n",
    "print(f\"(my_model) Got test AP: {test_eval_metrics_pt5.average_precision}\")\n",
    "print(f\"Greater than 0.025? {test_eval_metrics_pt5.average_precision >= 0.025}\")\n",
    "\n",
    "add_entry_to_auto_grader_data(auto_grader_data, [\"output\", \"part_5_a\", \"model_v2\", \"train_meta\"], train_metadata_pt5.to_pydict())\n",
    "add_entry_to_auto_grader_data(auto_grader_data, [\"output\", \"part_5_a\", \"model_v2\", \"test_eval_metrics\"], test_eval_metrics_pt5.to_pydict())\n",
    "add_entry_to_auto_grader_data(auto_grader_data, [\"output\", \"part_5_a\", \"model_v2\", \"test_metrics_op\"], test_metrics_op_pt5.to_pydict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission instructions\n",
    "Run the following cell below to generate + download your submission zip file: datac182_final_proj_submission.zip. You will submit this zip file to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: be sure that `root_folder` is defined correctly in previous cell\n",
    "from utils.utils import save_auto_grader_data\n",
    "from consts import STUDENT_SUBMISSION_OUTDIR\n",
    "import os\n",
    "outpath_autograder_data = os.path.join(STUDENT_SUBMISSION_OUTDIR, \"student_submission_data.pt\")\n",
    "save_auto_grader_data(auto_grader_data, outpath=outpath_autograder_data)\n",
    "print(f\"Saved auto_grader_data to: {outpath_autograder_data}\")\n",
    "\n",
    "os.chdir(root_folder)\n",
    "!pwd # make sure we are in the right dir\n",
    "!ls\n",
    "\n",
    "# Note: It's OK if the `rm` command fails, this is normal if this is your first\n",
    "#   time running this.\n",
    "!rm datac182_final_proj_submission.zip\n",
    "# zip everything (maintain dir structure) but exclude datasets (to keep filesize down)\n",
    "!zip -r datac182_final_proj_submission.zip . -x \"data/*\" \"__pycache__/*\" \"*/__pycache__/*\" \".git/*\"\n",
    "\n",
    "from google.colab import files\n",
    "outpath_zip = \"datac182_final_proj_submission.zip\"\n",
    "files.download(outpath_zip)\n",
    "print(f\"Finished downloading {outpath_zip}! Upload this zip file to Gradescope as your submission to run the autograder. The {outpath_zip} file will be in your browser's default download directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
